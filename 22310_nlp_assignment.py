# -*- coding: utf-8 -*-
"""22310_NLP Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIHaDGL8XRqsGd5D6jiIqkVE7Jhu0dOi
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from collections import Counter
from scipy.sparse import lil_matrix
from sklearn.decomposition import TruncatedSVD

df = pd.read_csv("labelled.csv")

# Keep only relevant columns
df = df[['Heading', 'Body']].dropna()
df['text'] = df['Heading'].astype(str) + " " + df['Body'].astype(str)

# Remove URLs
df['text'] = df['text'].replace(r'http\S+', '', regex=True).str.strip()

def tokenize(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)  # keep only alphabets
    return text.split()

# ------------------------------------------------
#  Build Vocabulary
# ------------------------------------------------
def build_vocab_limited(texts, max_vocab=5000):
    counter = Counter()
    for text in texts:
        counter.update(tokenize(text))

    most_common = counter.most_common(max_vocab)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    for word, _ in most_common:
        vocab[word] = len(vocab)
    return vocab

vocab = build_vocab_limited(df['text'].tolist(), max_vocab=5000)

def get_distinct_words():
    return list(vocab.keys())

# ------------------------------------------------
# Build Sparse Co-occurrence Matrix
# ------------------------------------------------
def build_cooccurrence_sparse(texts, vocab, window_size=4):
    vocab_size = len(vocab)
    matrix = lil_matrix((vocab_size, vocab_size), dtype=np.int32)

    for text in texts:
        tokens = tokenize(text)
        for i, token in enumerate(tokens):
            if token not in vocab:
                continue
            token_idx = vocab[token]

            start = max(0, i - window_size)
            end = min(len(tokens), i + window_size + 1)

            for j in range(start, end):
                if i != j:
                    neighbor = tokens[j]
                    if neighbor in vocab:
                        neighbor_idx = vocab[neighbor]
                        matrix[token_idx, neighbor_idx] += 1
    return matrix.tocsr()

co_matrix_sparse = build_cooccurrence_sparse(df['text'].tolist(), vocab, window_size=4)

# ------------------------------------------------
# Dimensionality Reduction
# ------------------------------------------------
def reduce_dimensions(matrix, k=2):
    svd = TruncatedSVD(n_components=k)
    reduced = svd.fit_transform(matrix)
    return reduced

embeddings_2d = reduce_dimensions(co_matrix_sparse, k=2)

def plot_embeddings(embeddings, vocab, num_points=50):
    plt.figure(figsize=(12, 8))
    words = list(vocab.keys())[:num_points]  # show only first N words
    for i, word in enumerate(words):
        x, y = embeddings[i, 0], embeddings[i, 1]
        plt.scatter(x, y)
        plt.text(x+0.01, y+0.01, word, fontsize=9)
    plt.title("Word Embeddings (Co-occurrence + SVD)")
    plt.show()

plot_embeddings(embeddings_2d, vocab, num_points=50)



